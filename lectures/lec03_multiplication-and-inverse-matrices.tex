% ==============================================================================
% Build Document Locally with Arara
% ==============================================================================

% arara: latexmk: {options: [-pv]}
% arara: indent: {overwrite: yes, silent: yes, cruft: build/}
\documentclass[../main.tex]{subfiles}

% ==============================================================================
% BEGIN DOCUMENT
% ==============================================================================
\begin{document}


% SECTION ======================================================================
\section{Multiplication and Inverse Matrices}


% SUBSECTION ===================================================================
\subsection{Overview}
% ==============================================================================
This lecture discusses the five approaches to matrix multiplication--by row \(\times\) column, by columns, by rows, by column \(\times\) row, and by blocks--, idea of a matrix inverse, and the Gauss-Jordan elimination technique that's commonly used to find such an inverse.


% SUBSECTION ===================================================================
\subsection{Multiplying matrices A and B by row times column}
% ==============================================================================
The first approach to multiplying matrices \(A\) and \(B\) is to start with a row of matrix \(A\) and take its \emph{dot product} with the corresponding column of matrix \(B\) to get a matrix entry of the resulting matrix \(C\).

\begin{adjustbox}{width = \columnwidth}
    \(
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
                   & \vdots &        &        \\
            a_{31} & a_{32} & \ldots & a_{3n} \\
                   & \vdots &        &
        \end{bmatrix}
    }_{\mathstrut \underset{m \times n}{A}}
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
                   & b_{14} &        \\
            \ldots & b_{24} & \ldots \\
                   & \vdots &        \\
                   & b_{n4} &
        \end{bmatrix}
    }_{\mathstrut \underset{n \times p}{B}}
    =
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
                   & \vdots &        \\
            \ldots & c_{34} & \ldots \\
                   & \vdots &
        \end{bmatrix}
    }_{\mathstrut \underset{m \times p}{C}}
    \)
\end{adjustbox}

In the illustration, the \(c_{34}\) element, i.e., the element located at row 3 and column 4 of matrix \(C\) is obtained by taking the dot product of the 3rd row of matrix \(A\) with the 4th column of matrix \(B\), i.e., \(\sum_{k=1}^{n} a_{3 k} b_{k 4}\).


% SUBSECTION ===================================================================
\subsection{Multiplying matrices A and B by columns}
% ==============================================================================
The second approach to multiplying matrices \(A\) and \(B\) is to start with a column of matrix \(B\) and use its elements to \emph{linearly combine} the columns of matrix \(A\) to obtain the corresponding column of the resulting matrix \(C\).

\begin{adjustbox}{width = \columnwidth}
    \renewcommand\arraystretch{1.5}
    \(
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            % Adapted from SO: https://tex.stackexchange.com/a/59614/180993
            \kern.6em\vline &       & \vline\kern.6em \\
            \kern.6em\vline & \dots & \vline\kern.6em \\
            \kern.6em\vline &       & \vline\kern.6em
        \end{bmatrix}
    }_{\mathstrut \underset{m \times n}{A}}
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
                  & \kern.2em\vline\kern.2em &       \\
            \dots & col \ j                  & \dots \\
                  & \kern.2em\vline\kern.2em &
        \end{bmatrix}
    }_{\mathstrut \underset{n \times p}{B}}
    =
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
                  & \kern.2em\vline\kern.2em &       \\
            \dots & A \times col \ j         & \dots \\
                  & \kern.2em\vline\kern.2em &       \\
        \end{bmatrix}
    }_{\mathstrut \underset{m \times p}{C}}
    \)
\end{adjustbox}


% SUBSECTION ===================================================================
\subsection{Multiplying matrices A and B by rows}
% ==============================================================================
The third approach to multiplying matrices \(A\) and \(B\) is to start with a row of matrix \(A\) and use its elements to linearly combine the rows of matrix \(B\) to obtain the corresponding row of the resulting matrix \(C\).
\[
    \renewcommand\arraystretch{2.5}
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \vdots                                                      \\
            % Adapted from SO: https://tex.stackexchange.com/a/59614/180993
            \rule[.5ex]{1.5em}{0.4pt} row \ i \rule[.5ex]{1.5em}{0.4pt} \\
            \vdots
        \end{bmatrix}
    }_{\mathstrut \underset{m \times n}{A}}
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \rule[2ex]{5em}{0.4pt} \\
            \vdots                 \\
            \rule[.5ex]{5em}{0.4pt}
        \end{bmatrix}
    }_{\mathstrut \underset{n \times p}{B}}
    =
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \vdots                                                              \\
            \rule[.5ex]{1.5em}{0.4pt}row \ i \times B \rule[.5ex]{1.5em}{0.4pt} \\
            \vdots
        \end{bmatrix}
    }_{\mathstrut \underset{m \times p}{C}}
\]


% SUBSECTION ===================================================================
\subsection{Multiplying matrices A and B by column times row}
% ==============================================================================
The fourth approach to multiplying matrices \(A\) and \(B\) is to generate a bunch of matrices by multiplying the columns of matrix \(A\) with the corresponding rows of matrix \(B\) and adding them together.
\[
    \renewcommand\arraystretch{1.5}
    \equalto{
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \kern.6em\vline &        & \kern.2em\vline\kern.2em &        & \kern.6em\vline \\
            col \ 1         & \ldots & col \ i                  & \ldots & col \ n         \\
            \kern.6em\vline &        & \kern.2em\vline\kern.2em &        & \kern.6em\vline
        \end{bmatrix}
    }_{\mathstrut \underset{m \times n}{A}}
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \rule[.5ex]{1.5em}{0.4pt}row \ 1 \rule[.5ex]{1.5em}{0.4pt} \\
            \vdots                                                     \\
            \rule[.5ex]{1.5em}{0.4pt}row \ j \rule[.5ex]{1.5em}{0.4pt} \\
            \vdots                                                     \\
            \rule[.5ex]{1.5em}{0.4pt}row \ n \rule[.5ex]{1.5em}{0.4pt}
        \end{bmatrix}
    }_{\mathstrut \underset{n \times p}{B}}}
    {
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \horzbar & \horzbar & \horzbar \\
            \horzbar & \horzbar & \horzbar \\
            \horzbar & \horzbar & \horzbar
        \end{bmatrix}
    }_{\mathstrut \underset{m \times p}{C_1}}
    +
    \ldots
    +
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \horzbar & \horzbar & \horzbar \\
            \horzbar & \horzbar & \horzbar \\
            \horzbar & \horzbar & \horzbar
        \end{bmatrix}
    }_{\mathstrut \underset{m \times p}{C_i}}
    +
    \ldots
    +
    \underbracket[0pt][0pt]{
        \begin{bmatrix}
            \horzbar & \horzbar & \horzbar \\
            \horzbar & \horzbar & \horzbar \\
            \horzbar & \horzbar & \horzbar
        \end{bmatrix}
    }_{\mathstrut \underset{m \times p}{C_n}}}
\]
Let's illustrate this matrix multiplication approach with the following example:
\begingroup
\allowdisplaybreaks
\begin{align*}
    \begin{bmatrix}
        2 & 7 \\
        3 & 8 \\
        4 & 9
    \end{bmatrix}
    \begin{bmatrix}
        1 & 6 \\
        0 & 0
    \end{bmatrix}
     & =
    \begin{bmatrix}
        2 \\
        3 \\
        4
    \end{bmatrix}
    \begin{bmatrix}
        1 & 6
    \end{bmatrix}
    +
    \begin{bmatrix}
        7 \\
        8 \\
        9
    \end{bmatrix}
    \begin{bmatrix}
        0 & 0
    \end{bmatrix} \\
     & =
    \begin{bmatrix}
        2 \times 1 & 2 \times 6 \\
        3 \times 1 & 3 \times 6 \\
        4 \times 1 & 4 \times 6
    \end{bmatrix}
    +
    \begin{bmatrix}
        7 \times 0 & 7 \times 0 \\
        8 \times 0 & 8 \times 0 \\
        9 \times 0 & 9 \times 0
    \end{bmatrix} \\
     & =
    \begin{bmatrix}
        2 & 12 \\
        3 & 18 \\
        4 & 24
    \end{bmatrix}
    +
    \begin{bmatrix}
        0 & 0 \\
        0 & 0 \\
        0 & 0
    \end{bmatrix}
    =
    \begin{bmatrix}
        2 & 12 \\
        3 & 18 \\
        4 & 24
    \end{bmatrix}
\end{align*}
\endgroup
The resulting matrix \(\begin{bsmallmatrix} 2 & 12 \\ 3 & 18 \\ 4 & 24 \end{bsmallmatrix}\) is a special matrix with the following properties:
\begin{itemize}
    \item all of its rows lie on the line through the vector \(\begin{bsmallmatrix} 1 \\ 6 \end{bsmallmatrix}\), and
    \item  all of its columns lie on the line through the vector \(\begin{bsmallmatrix} 2 \\ 3 \\ 4 \end{bsmallmatrix}\)
\end{itemize}
In other words, the \emph{row space} and the \emph{column space} of this matrix is a line. % TODO: Add reference to chapters where these terms are defined.


% SUBSECTION ===================================================================
\subsection{Multiplying matrices A and B by blocks}
% ==============================================================================
The fifth approach to multiplying matrices \(A\) and \(B\) is to cut the matrices into blocks and then multiply the block rows of \(A\) with the block columns of \(B\) to get the corresponding blocks of the resulting matrix \(C\).

\begin{adjustbox}{width = \columnwidth}
    \(
    \renewcommand\arraystretch{1.5}
    % Adapted from SO: https://tex.stackexchange.com/a/230505/180993
    \sbox0{\(\begin{matrix}1 & 0 \\ 0 & 1\end{matrix}\)}
    \underbracket[0pt][0pt]{
        \left[
            \begin{array}{c|c}
                \vphantom{\usebox{0}}\makebox[\wd0]{\large \(A_1\)} & \makebox[\wd0]{\large \(A_2\)} \\
                \hline
                \vphantom{\usebox{0}}\makebox[\wd0]{\large \(A_3\)} & \makebox[\wd0]{\large \(A_4\)}
            \end{array}
            \right]
    }_{\mathstrut \underset{m \times n}{A}}
    \underbracket[0pt][0pt]{
        \left[
            \begin{array}{c|c}
                \vphantom{\usebox{0}}\makebox[\wd0]{\large \(B_1\)} & \makebox[\wd0]{\large \(B_2\)} \\
                \hline
                \vphantom{\usebox{0}}\makebox[\wd0]{\large \(B_3\)} & \makebox[\wd0]{\large \(B_4\)}
            \end{array}
            \right]
    }_{\mathstrut \underset{n \times p}{B}}
    =
    \underbracket[0pt][0pt]{
        \left[
            \begin{array}{c|c}
                \vphantom{\usebox{0}}\makebox[\wd0]{\large \(C_1\)} & \makebox[\wd0]{\large \(C_2\)} \\
                \hline
                \vphantom{\usebox{0}}\makebox[\wd0]{\large \(C_3\)} & \makebox[\wd0]{\large \(C_4\)}
            \end{array}
            \right]
    }_{\mathstrut \underset{m \times p}{C}}
    \)
\end{adjustbox}
The first block, \(C_1\), is the dot product of the first block row of matrix \(A\) and the first block column of matrix \(B\), i.e.,  \(A_1\times B_1 + A_2 \times B_3\), and other blocks are obtained similarly.


% SUBSECTION ===================================================================
\subsection{Matrix inverses}
% ==============================================================================
Inverse of a matrix \(A\) is a matrix that when pre-multiplied or post-multiplied to \(A\) returns an \emph{identity matrix}. Identity matrix is a \emph{square matrix} (same number of rows and columns) and has \(1\)s along the diagonal and \(0\)s off the diagonal. A \(3 \times 3\) identity matrix is represented as follows.
\[
    \underset{3 \times 3}{I}
    =
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\]
Also, depending on whether it's pre-multiplied or post-multiplied, it's also called the \emph{left inverse} of \(A\) (sitting on the left of \(A\)) or the \emph{right inverse} of \(A\) (sitting on the right of \(A\)).
\[
    A^{-1}A = I = AA^{-1} \text{(if the inverse exists)}
\]
For rectangular matrices, left and right inverses are not the same. In fact, the rules of matrix multiplication won't allow that. However, they are the same for square matrices.
\vspace{0.5em}

If there exists a non-zero vector \(x\) that when post-multiplied to \(A\) results in a zero vector, then the matrix \(A\) does not have an inverse and it's called \emph{non-invertible} or \emph{singular matrix}. Otherwise, it's called \emph{invertible} or \emph{non-singular matrix}.
\vspace{0.5em}

For example, the matrix \(\begin{bsmallmatrix} 1 & 3 \\ 2 & 6 \end{bsmallmatrix}\) does not have an inverse because there exists a non-zero vector \(\begin{bsmallmatrix*}[r] 3 \\ -1 \end{bsmallmatrix*}\) that when post-multiplied to the matrix results in a zero vector:
\[
    \begin{bmatrix*}[r]
        1 & 3 \\
        2 & 6
    \end{bmatrix*}
    \begin{bmatrix*}[r]
        3 \\
        -1
    \end{bmatrix*}
    =
    \begin{bmatrix*}[r]
        0 \\
        0
    \end{bmatrix*}
\]


% SUBSECTION ===================================================================
\subsection{Gauss Jordan elimination}
% ==============================================================================
Gauss-Jordan elimination is an extended form of Gaussian elimination that not only cleans the elements below the pivots, but also above it. This method is generally used to find the \emph{inverse} of a matrix, if it exists.
\vspace{0.5em}

Let's first look at the big picture and then find the inverse of
\[
    \begin{bmatrix}
        1 & 3 \\
        2 & 7
    \end{bmatrix}
\]
using the Gauss-Jordan elimination technique.
\vspace{0.5em}

From the definition of an inverse, it follows that to obtain the inverse of the \(2 \times 2\) example matrix, a \(2 \times 2\) matrix needs to be identified that when post-multiplied (or pre-multiplied) to the example matrix returns an identity matrix:
\[
    \begin{bmatrix}
        1 & 3 \\
        2 & 7
    \end{bmatrix}
    \begin{bmatrix}
        a & c \\
        b & d
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}
\]
Interestingly, solving the above equation is like solving two equations at once:
\[
    \begin{bmatrix}
        1 & 3 \\
        2 & 7
    \end{bmatrix}
    \begin{bmatrix}
        a \\
        b
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 \\
        0
    \end{bmatrix}
    \text{and}
    \begin{bmatrix}
        1 & 3 \\
        2 & 7
    \end{bmatrix}
    \begin{bmatrix}
        c \\
        d
    \end{bmatrix}
    =
    \begin{bmatrix}
        0 \\
        1
    \end{bmatrix}
\]
Now let's follow the steps of Gauss-Jordan elimination to find the inverse of the example matrix.

\textbf{Step 0:} Tack on the identity matrix to the example matrix and create an \emph{augmented matrix}:
\[
    \begin{bmatrix}
        1 & 3 & \aug & 1 & 0 \\
        2 & 7 & \aug & 0 & 1
    \end{bmatrix}
\]
\vspace{0.5em}

\textbf{Step 1:} Mark the first pivot at \((1, 1)\) and clean off every element below it:
\[
    \begin{gmatrix}[b]
        \boxed{1} & 3 & \BAR & 1 & 0 \\
        2 & 7 & \BAR & 0 & 1
        \rowops
        \add[-2]{0}{1}
    \end{gmatrix}
    \xrightarrow{E_{21}}
    \begin{gmatrix}[b]
        \boxed{1} & 3 & \BAR & 1 & 0 \\
        0 & 1 & \BAR & -2 & 1
    \end{gmatrix}
\]
\vspace{0.5em}

\textbf{Step 2:} Mark the second pivot at \((2, 2)\). Now, there is nothing to clean below the second pivot and Gauss would have stopped. But Jordan asked to move upward to clean elements above it:

\begin{adjustbox}{width = \columnwidth}
    \(
    \begin{gmatrix}[b]
        \boxed{1} & 3 & \BAR & 1 & 0 \\
        0 & \boxed{1} & \BAR & -2 & 1
        \rowops
        \add[-3]{1}{0}
    \end{gmatrix}
    \xrightarrow{E_{12}}
    \begin{gmatrix}[b]
        \boxed{1} & 0 & \BAR & 7 & -3 \\
        0 & \boxed{1} & \BAR & -2 & 1
    \end{gmatrix}
    \)
\end{adjustbox}
\vspace{0.5em}

As evident from the steps shown above, a bunch of elimination matrices operated on the augmented matrix and turned the example matrix (say \(A\)) into an identity matrix. However, it's not immediately obvious how the identity matrix got transformed into the inverse of \(A\).
\vspace{0.5em}

Let's see why that happened. The operation \(E\begin{bmatrix} A & \aug & I \end{bmatrix}\) resulted in another augmented matrix \(\begin{bmatrix} I & \aug & E \end{bmatrix}\). Comparing both sides gives \(EA = I\) and from the definition of an inverse it follows that \(E = A^{-1}\).


% ==============================================================================
% END DOCUMENT
% ==============================================================================
\end{document}
